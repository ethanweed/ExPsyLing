# 9781841693408 copy


#### [Page 4](highlights://Harley_Chapter5#page=4)

> We tend to categorise, and perhaps think, at the basic level;
> there’s a large loss in distinctiveness as we go from the basic
> level to the superordinate category, but not much to be gained
> most of the time by making unnecessarily fine distinctions
> between subordinates below

> Basic-level objects have several psychological advantages:
> children usually

#### [Page 5](highlights://Harley_Chapter5#page=5)

> learn basic-level names first

> it’s the highest level at which we can form a mental image (try
> forming a mental picture of “animal” without thinking of
> something more specific)

> people can find most things to say about basic-level things

> we process basic-level names more quickly than those at other
> levels

#### [Page 6](highlights://Harley_Chapter5#page=6)

> Collins and Quillian (1969) realised that once you’ve specified
> information at one level, you don’t need to do it again at a
> lower one. They presented a model of semantic memory known as a
> semantic network.

> Think about how we might verify a statement such as “a canary is
> a bird”. We’d start off at the CANARY node, and travel up to the
> BIRD node. The two nodes are connected by an ISA link, so the
> statement is true. What about “a canary is an animal”? Just the
> same, only this time we have longer to travel to establish a
> connection. So the crucial pre- diction is that it should take
> longer to verify “a canary is an animal” than “a canary is a
> bird”.

#### [Page 7](highlights://Harley_Chapter5#page=7)

> Collins and Quillian tested these predictions using a sentence
> verification task. This task is very simple: you present people
> with a sentence on a computer screen, such as “a canary has
> wings”, and ask them to press one key if the statement is true
> and another if it’s false, and you measure how long it takes
> them to make the decision. Obviously it’s going to take people
> some time to read the sentences and some time to press the key,
> but these should be constant across sentences. Any differences
> in response time should therefore reflect differences in
> decision time.

> although no one now thinks this is how we store all information
> about word meaning, the basic idea that meaning is represented
> by the interconnection of concepts is still very much alive.

> If two things are related in some way, we find a false sentence
> more difficult to reject than one in which the two things are
> unrelated: A pine is a church. A pine is a flower.

> And we’re faster at dealing with items that are more typical of
> their category than ones that are not, a result called the
> typicality effect:

#### [Page 8](highlights://Harley_Chapter5#page=8)

> A penguin is a bird. A robin is a bird.

#### [Page 9](highlights://Harley_Chapter5#page=9)

> the definitions I’ve discussed so far do have one subtle but
> important feature in common: they try to explain the meaning of
> something in terms of combinations of simpler units of meaning.

#### [Page 10](highlights://Harley_Chapter5#page=10)

> We can take this idea that meaning is best represented by
> combinations of smaller units of meaning much further so that it
> acts as the basis for a theory of semantic memory. We call such
> approaches decompositional theories.

#### [Page 11](highlights://Harley_Chapter5#page=11)

> The other great advantage of semantic features is that they give
> us a way to build up the meaning of sentences.

#### [Page 12](highlights://Harley_Chapter5#page=12)

> famous example is “game” (Wittgenstein, 1953)

> this apparent problem isn’t really a problem at all for a
> feature-based theory; we merely dispense with the notion of
> necessary defining features, and look at the total amount of
> overlap.


